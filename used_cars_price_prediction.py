# -*- coding: utf-8 -*-
"""Copy of Capstone_Project_Reference_Notebook_Full_Code_Used_Cars_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l3refVcvTBB1UuKxe6XOO-tzOoD3F7QZ

# **Used Cars Price Prediction**
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Problem Definition**

### **The Context:**

 - Why is this problem important to solve?

### **The objective:**

 - What is the intended goal?

### **The key questions:**

- What are the key questions that need to be answered?

### **The problem formulation**:

- What is it that we are trying to solve using data science?

### **Data Dictionary**

**S.No.** : Serial Number

**Name** : Name of the car which includes Brand name and Model name

**Location** : The location in which the car is being sold or is available for purchase (Cities)

**Year** : Manufacturing year of the car

**Kilometers_driven** : The total kilometers driven in the car by the previous owner(s) in KM

**Fuel_Type** : The type of fuel used by the car (Petrol, Diesel, Electric, CNG, LPG)

**Transmission** : The type of transmission used by the car (Automatic / Manual)

**Owner** : Type of ownership

**Mileage** : The standard mileage offered by the car company in kmpl or km/kg

**Engine** : The displacement volume of the engine in CC

**Power** : The maximum power of the engine in bhp

**Seats** : The number of seats in the car

**New_Price** : The price of a new car of the same model in INR 100,000

**Price** : The price of the used car in INR 100,000 (**Target Variable**)

### **Loading libraries**
"""

#import necessary libraries for data analysis

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#import libraries for building a linear regression model

import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

#import ignore warnings
import warnings
warnings.filterwarnings('ignore')

"""### **Let us load the data**"""

#import dataset
car = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Capstone Project/used_cars.csv')

"""## Data Overview

- Observations
- Sanity checks
"""

#lets see what we're working with :)

print(f'We have {car.shape[0]} rows and {car.shape[1]} columns in our data set.\n')

#car.head()

'''
after viewing .head(), my initial observations would be
to drop the serial number column as it does not provide any
feature information, as well as to create a new column that
isolates the brand of car from the model name (i.e. only hyundai instead
of hyundai creta..). Next I'll check for duplicate and missing values and
other discrepancies in the dataset before moving on to EDA.
'''
#create a copy of the ds so our changes dont copy to the original

df = car.copy(deep = True)
df = df.drop('S.No.', axis = 1) #drop 'S.No' col

df.info() #check data types

print('\nit looks like cols Mileage, Engine, Power, Seats, New_price and Price')
print('have some missing values, lets see how many there are:')
print('\nMissing values by column:')
print(df.isnull().sum()) #check missing val for each col
total_missing = df.isnull().sum().sum() #check total

print('\n Percentage of Missing Values:')
print(round((df.isnull().sum() / df.shape[0] * 100), 2)) #check % of missing  vals per col

#The majority of missing values comes from the New_Price col at ~86%
#next largest is Price (~17%) and then Power (~2%)

print(f'\nWe have a total of {total_missing} values missing in our dataset,')
print('with a majority being from the new_price column.\n')

duplicate = df[df.duplicated()] #check for duplicates
df[df.duplicated(keep = False)] #view the duplicates

'''
Observation: There is one set of duplicates. In the original dataset cars these
two rows have different serial numbers but I think its because they were
entered on two separate occasions because all the other information such as
mileage, dealership, model etc are all the same so I'm going to drop one of the
duplicates. If I was working this job in reality I would first contact the dealership
to determine whether this was a duplicate before dropping.
'''
df.drop_duplicates(inplace = True) #drop duplicate
df.shape #check our work to see we've dropped the duplicated row, now have 7253 entries

print('View nunique values:')
print(df.nunique()) #check for unique values in each column

#create a new 'Brand' feature that splits Name of car from maker
#this way we can make brand an additional feature for our dataset

print('\nAdding a Brand column to our dataset:')
brand = df['Name'].str.split(' ', n = 1, expand = True)

df['Brand'] = brand[0] #append to dataset
df['Brand'].value_counts() #check work to make sure new col is created properly

df.head() #view our new dataset

"""# Observations:

- Our dataset has 7253 records and 14 columns
- The 'S. No.' column is an ID column for each car and is not likely to have any influence on our dependent variable ('Price') so I will drop it
- Columns [Mileage, Engine, Power, Seats, New_price, Price] contain missing values
- The majority of missing values are from the 'New_price' column at ~86% and the next largest being the 'Price' column at ~17%
- The 'Name' column has so many unique values that it will be difficult to work with as is, so I created a new column called 'Brand' to store the brand info extracted from each record in 'Name'
- Dropped the one duplicate found

## **Exploratory Data Analysis**

- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.

**Questions:**

1. What is the summary statistics of the data? Explore summary statistics for numerical variables and the categorical variables
2. Find out number of unique observations in each category of categorical columns? Write your findings/observations/insights
3. Check the extreme values in different columns of the given data and write down the observtions? Remove the data where the values are un-realistic
"""

print('\nView summary stats:')
print(df.describe().T) #view summary stats for numerical cols

print('\nthe maximum mileage in our summary statistics is 6,500,000')
print('which is impossible and likely a typo. Lets view the column:')

df['Kilometers_Driven'].sort_values(ascending = False).head(10)
print(df.loc[df['Kilometers_Driven'] == 6500000]) #view record

print('\nIts definitely a typo, lets change it 650,000 km instead.')

df['Kilometers_Driven'] = df['Kilometers_Driven'].replace(6500000, 650000) #replace value
print('\nView new Kilometers_Driven col after replacing value:')
df['Kilometers_Driven'].sort_values(ascending = False).head(10) #check work

"""# Outlier Treatment

- The maximum value for the 'Kilometers_Driven' column was 6,500,000 which is impossible and likely a typo. This value was changed to 650,000

"""

#split cols into categorical and numerical for analysis

df_cat = df[['Brand', 'Name', 'Location', 'Fuel_Type', 'Transmission', 'Owner_Type']]

#decided to put year in df_num even though its technically categorical because it can
#serve as a useful predictive variable in regression analysis
df_num = df[['Year', 'Kilometers_Driven', 'Mileage', 'Engine', 'Power', 'Seats', 'New_price', 'Price']]

#df_c = df.select_dtypes(include = 'object') #can also use this to quickly parse df by dtypes
#df_n = df.select_dtypes(exclude = 'object')

print(df_cat.nunique())
print(df_num.nunique())

print(df_cat.describe().T)
df_num.describe().T

# Printing the number of occurrences of each unique value in each categorical col
for col in df_cat:
    print(df[col].value_counts(1))
    print("-" * 50)

"""# Observations

- Brand, Name, Location, Fuel_Type, Transmission and Owner_Type are of the object data type and the rest are of the numeric datatype
- Year could fit into either category technically but I've place it in the numeric category as it can serve as a useful predictive variable in regression analysis
- This data set contains in total 33 brands, 11 locations, 5 fuel types, 2 transmission types and 4 owner types, as well as 2041 names. None of our categorical columns contain missing values
- Within the dataset, the following are the most popular of our categorical items: Brand - Maruti, Name - Mahindra XUV500 W8 2WD, Location - Mumbai, Fuel_Type - Diesel, Transmission - Manual, Owner_Type - first
- Cars in this dataset were made in years spanning 1996 to 2019
- The average amount of km driven  is ~57,000
- Both the average and median car Mileage is ~18 kmpl
- The average power displacement is ~1616 bhp
- The average and most frequent amount of seats in a car is 5
- On average, it appears that the Price of a car is sold for is around half of what its original New_price is. It is important to note however that this may not be a reliable observation as these two columns contain the majority of our missing values
- On average, the sale price of used cars is ~947,000 INR

## **Univariate Analysis**

**Questions:**

1. Do univariate analysis for numerical and categorical variables?
2. Check the distribution of the different variables? is the distributions skewed?
3. Do we need to do log_transformation, if so for what variables we need to do?
4. Perfoem the log_transformation(if needed) and write down your observations?
"""

#check for distribution of categorical col in data

#plot only the most 25 frequently appearing brands for ease of data presentation
print('Brand')
fig = plt.figure(figsize = (18, 6))
sns.countplot(x = 'Brand', data = df_cat, color = 'blue', order = df_cat['Brand'].value_counts().iloc[:25].index);
plt.xticks(rotation = 90);
plt.show()

#remove Brand and Name from this because they are so many different values in both categories
cat_vals = ['Location', 'Fuel_Type', 'Transmission', 'Owner_Type', 'Year']
for col in cat_vals:
  print(col)
  fig = plt.figure(figsize = (13, 6))
  sns.countplot(x = col, data = df, color = 'blue', order = df[col].value_counts().index);
  plt.show()

#Because there are so many names plotted only Name records with a frequency of >15
print('Name')
df['Name'].value_counts()[:61].sort_values(ascending = False).plot(kind = 'bar', figsize = (15, 6))
plt.show()

"""#  Observations

- Maruti, Hyundai and then Honda are the most popular car brands in this dataset
- Diesel and Petrol make up the majority (~98%) of all fuel types
- Around 82% of cars within this dataset are on their first round of ownership. It is extremely rare for a car to have a third or higher ownership
- There is a fairly even distribution of car locations although the locations Mumbai, Hyderabad, Coimbatore, Kochi, Pune, Delhi and Kolkata are most popular. This makes sense as they are large cities
- Most of the cars in this dataset were made from 2011 to 2017, with the most frequent being years 2015 and 2014
- There is a fairly even distribution of different car names irrespective of Brand. Name is not likely to be a useful predictive column for our analysis because of this and I may consider dropping it
"""

#check for distribution and outliers for each numerical col in data

col_names = list(df_num.columns)
print(col_names)
'''
fig, ax = plt.subplots(1, 2, figsize = (15, 6))
sns.histplot(x = 'Mileage', data = df_num, ax = ax[0], kde = True);

sns.boxplot(x = df['Mileage'], ax = ax[1]);
plt.show()
'''
for col in col_names:

  print(col)
  print('Skew :', round(df_num[col].skew(),2))
  fig, ax = plt.subplots(1, 2, figsize = (15, 6))
  sns.histplot(x = df_num[col], ax = ax[0], kde = True);

  sns.boxplot(x = df[col], ax = ax[1]);
  plt.show()

#check % of outliers in numerical columns

for k, v in df_num.items():
        q1 = v.quantile(0.25)
        q3 = v.quantile(0.75)
        irq = q3 - q1
        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]
        perc = np.shape(v_col)[0] * 100.0 / np.shape(df_num)[0]
        print("Column %s outliers = %.2f%%" % (k, perc))

"""# Observations

- A majority of cars lie within 50,000 to 100,000 km driven but there are several very high outliers
- Many of our numerical columns have a good amount of outliers, usually with higher values (Kilometers_Driven, Engine, Power, Seats, New_price, and Price)
- Kilometers_Driven, Engine, Power, Seats, New_price and Price are all highly skewed towards the right side (>1). Year has a somewhat negative skew towards the left (-0.84)

**Questions:**

1. Plot a scatter plot for the log transformed values(if log_transformation done in previous steps)?
2. What can we infer form the correlation heatmap? Is there correlation between the dependent and independent variables?
3. Plot a box plot for target variable and categorical variable 'Location' and write your observations?
"""

sns.heatmap(df.corr(numeric_only = True), annot = True, vmin = -1, vmax = 1, fmt = '.2f');
plt.show()

"""# Observations

**Variables with a strong positive correlation (> 0.7):**

- Engine & Power are strongly correlated to the point it may be worth treating them as one variable. A stronger engine has greater power
- Engine & New_Price, a better engine likely boosts the price a new car can be sold at
- Power & New_Price, higher power likely also boosts the price a new car can be sold at
- Power & Price, power seems to have staying power even in relation to resale price
- New_Price and Price, these are also strongly correlated which makes sense as a car that is originally sold at a high price is more likely to still retain a better resale price. That said, New_Price is the variable with the largest amount of null values (~86%) so I may end up having to drop it despite its relevance, or highly modify it


**There are no numerical variables with a strong negative correlation (< -0.7)**

Other notes:
- Engine & Price have a slight (+) correlation (.66). A better engine likely gives a slight boost to a car's resale value.
- Mileage & Engine and Mileage & Power have a slight (-) correlations (-.59 and -.53 respectively). This makes sense as Engine & Power are highly correlated with each other. It could be that cars often sacrifice Engine and Power to achieve better Mileage.

**Note:** While there are some strong correlations between Price and our numerical values there is not as many as you would normally expect, which indicates that there is likely a lot of predictive power held within our categorical values as well

"""

df_cat_list = ['Brand', 'Location', 'Fuel_Type', 'Transmission', 'Owner_Type']

for col in df_cat_list:
  sns.barplot(x = 'Price', y = col, data = df)
  plt.show()

"""# Observations

- Cars from more expensive brands such as Lamborghini and Bentley have a higher resale price
-Cars sold from the Coimbatore and Bangalore locations sell at higher prices. Is it that the locations themselves price cars higher or is that they are more likely to sell luxury brands?
- Diesel and Electric cars have higher prices. It could be again that they are higher in demand or perhaps contain more expensive brands
- Automatic cars resale for higher than Manual
- The more owners a car has the lower the resale price it is likely to be sold for. As an increase in owners can be interpreted as greater wear and tear and use on the car, it makes sense that the cars value is implied to decrease every time it changes hands
"""

def stacked_barplot(data, predictor, target):
  """
    Print the category counts and plot a stacked bar chart

    data: dataframe
    predictor: independent variable
    target: target variable
  """
  count = data[predictor].nunique()
  sorter = data[target].value_counts().index[-1]
  tab1 = pd.crosstab(data[predictor], data[target], margins = True).sort_values(
      by = sorter, ascending = False
  )
  print(tab1)
  print("-" * 120)
  tab = pd.crosstab(data[predictor], data[target], normalize = "index").sort_values(
      by = sorter, ascending = False
  )
  tab.plot(kind = "bar", stacked = True, figsize = (count + 1, 5))
  plt.legend(
      loc = "lower left",
      frameon = False,
  )
  plt.legend(loc = "upper left", bbox_to_anchor = (1, 1))
  plt.show()

stacked_barplot(df, 'Location', 'Brand')

stacked_barplot(df, 'Location', 'Owner_Type')

stacked_barplot(df, 'Location', 'Transmission')

"""# Observations

- There is no strong pattern in which locations sell which Brands that could explain why certain locations tend to have higher priced cars
- Kolkata and Kochi are the locations that sell the most cars on their first ownership, but all locations have a more than half their cars on their first ownership. In addition, Bangalore ranks fairly high on the pricing yet has the largest amount of second and above owned cars
- all locations have a majority of manual transmission cars
- From a glance, the Brand, Ownership_Type and Transmission features cannot fully explain why certain locations tend to have higher priced cars
"""

fig = plt.figure(figsize = (18, 6))
#lineplots are good when the x variable is time
sns.lineplot(x = 'Year', y = 'Price', data = df, color = 'blue');
plt.show()

#our correlation matrix shows that Engine, Power and New_price have a >.5 correlation
#with Price so lets transform and plot those

from sklearn import preprocessing
# Let's scale the columns before plotting them against MEDV
min_max_scaler = preprocessing.MinMaxScaler()
column_sels = ['Engine', 'Power', 'New_price']
x = df.loc[:,column_sels]
y = df['Price']
x = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)
fig, axs = plt.subplots(ncols=3, nrows=1, figsize=(20, 10))
index = 0
axs = axs.flatten()
for i, k in enumerate(column_sels):
    sns.regplot(y=y, x=x[k], ax=axs[i])
plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)

#remove skewness from data

df['Price'] = np.log1p(df['Price'])
for col in x.columns:
  if np.abs(x[col].skew()) > 0.3:
    df[col] = np.log1p(x[col])

sns.set_theme(style = 'ticks')

#Initialize the figure w/ a logarithmic x axis
f, ax = plt.subplots(figsize = (7,6))


sns.boxplot(data = df, x = 'Price', y = 'Location', hue = 'Location', width = .6, whis= [0, 100], palette = 'vlag')
#sns.stripplot(data = df, x = 'Price', y = 'Location', size = 4, color = '.3')

ax.xaxis.grid(True)
ax.set(ylabel = '')
sns.despine(trim = True, left = True)

"""# Observations

- price correlates somewhat positively with year with some variation. Newer cars likely have better resale value
- log transformed multiple numerical columns that showed a strong positive correlation with the target variable (Engine, Power,  New_price)
- the boxplot of Location v. Price indicates that Coimbatore and Bangalore locations have cars with higher prices however there is so much variation within each location that it is hard to make any definitive statements with this plot

### **Feature Engineering**

**Think about it:** The `Name` column in the current format might not be very useful in our analysis.
Since the name contains both the brand name and the model name of the vehicle, the column would have too many unique values to be useful in prediction. Can we extract that information from that column?

* **Hint:** With 2041 unique names, car names are not going to be great predictors of the price in our current data. But we can process this column to extract important information for example brand name.
"""

#note already created a brand column during data clean up, please see above
#I am using the Name column to fill some missing data and will drop it afterwards prior to analysis

"""### **Missing value treatment**"""

#We have a lot of missing data within this dataset
#so lets check the % missing in each column and
#also treat some of those missing vals
pd.DataFrame(
    {
        'Count' : df.isnull().sum()[df.isnull().sum() > 0], #view # of missing vals per col
        'Percentage' : ((df.isnull().sum()[df.isnull().sum() > 0] / df.shape[0]) *100).round(2)

    }
)

#The majority of missing values comes from the New_Price col at ~86%
#next largest is Price (~17%) and then Power (~2%)

"""- New_Price and Price have the highest number of missing values
- Mileage has the least number of missing values
- Engine and Seats have an almost equal percentage of missing values, with Power having slightly more
"""

df.loc[df['Mileage'].isnull() == True] #view rows with missing values

df.loc[df['Engine'].isnull() == True, 'Name'].value_counts(dropna = False)
df.loc[df['New_price'].isnull() == True, 'Power'].value_counts(dropna = False)
df.loc[df['New_price'].isnull() == True, 'Price'].value_counts(dropna = False)
df.loc[df['New_price'].isnull() == True, 'Seats'].value_counts(dropna = False)

"""- All NA values in Seats and Power are also missing New_price, and a majority of records that are missing Price are also missing New_price (1051 out of 1233). There is no overlap between missing Mileage or Engine and missing New_Price values"""

#view missing values by Name column using the assumption that if it is the same car and model,
#it should have similar values for Engine, Power and Seats
df.loc[df['Engine'].isnull() == True, 'Name'].value_counts(dropna = False)
df.loc[df['Power'].isnull() == True, 'Name'].value_counts(dropna = False)
df.loc[df['Seats'].isnull() == True, 'Name'].value_counts(dropna = False)

df_select = df.groupby(['Name'])[['Engine', 'Power', 'Seats']].mean()
df_select

df['Engine'] = df['Engine'].fillna(value = df.groupby(['Name'])['Engine']. transform('mean'))
df.loc[df['Engine'].isnull() == True] #view rows with missing values
df['Power'] = df['Power'].fillna(value = df.groupby(['Name'])['Power']. transform('mean'))
df['Seats'] = df['Seats'].fillna(value = df.groupby(['Name'])['Seats']. transform('mean'))

#we have decreased some of the missing values using this method but not all, so we widen
#our criteria and begin to fill missing values using  Brand

df['Engine'] = df['Engine'].fillna(
    value = df.groupby(['Brand'])['Engine'].transform('mean')
)
df['Power'] = df['Power'].fillna(
    value = df.groupby(['Brand'])['Power'].transform('mean')
)
df['Seats'] = df['Seats'].fillna(
    value = df.groupby(['Brand'])['Seats'].transform('mean')
)
df['Mileage'] = df['Mileage'].fillna(
    value = df.groupby(['Brand'])['Mileage'].transform('mean')
)

#view 2 remaining NA values in Power col
print(df[df['Power'].isnull() == True])

#View any other cars with the same brand
print(df[df['Brand'] == 'Smart'])
df[df['Brand'] == 'Hindustan']

#Because these are only two datapoints and they have several missing values in other columns as well
#and no other records share the same brands to base them off of, we will drop these

df[df['Power'].isnull()] #view rows with missing data, it is 915 and 6216

df = df.drop([915, 6216]).reset_index(drop = True) #drop missing values

#check our work to confirm that we have filled all missing values
#for Mileage, Power, Seats and Engine
pd.DataFrame(
    {
        'Count' : df.isnull().sum()[df.isnull().sum() > 0], #view # of missing vals per col
        'Percentage' : ((df.isnull().sum()[df.isnull().sum() > 0] / df.shape[0]) *100).round(2)

    }
)

#unfortunately for New_price there is such a large amount of missing values
#that I am going to drop the column although I'd love to hear a mentors
#thoughts on how to handle this in the future bc there was strong correlation
#between New_price and Price our dependent variable
#similarly it is not recommended to impute on your target variable so
#I am going to drop the rows that are missing Price data and see how it goes
#cutting out 17% of the data is not ideal but I don't want to cause an issue
#in the model by imputing

df =df.drop(['New_price'], axis = 1)
df_new = df[df['Price'].notnull()]
df_new.shape
df_new.head()

#drop the name column now to prep our data for model building
df_new = df_new.drop('Name', axis = 1) #dropped name
df_new.head()

#check to confirm there are no missing values in our new df
pd.DataFrame(
    {
        'Count' : df_new.isnull().sum()[df_new.isnull().sum() > 0], #view # of missing vals per col
        'Percentage' : ((df_new.isnull().sum()[df_new.isnull().sum() > 0] / df.shape[0]) *100).round(2)

    }
)

"""# Notes

-  finished preprocessing data
- filled missing values for columns Mileage, Engine, Power, and Seats
- dropped New_price column due to its large amount of missing values (~86% of the column)
- removed records with missing Price values because imputing the target variable is likely to create an incorrect model

## **Important Insights from EDA and Data Preprocessing**

What are the the most important observations and insights from the data based on the EDA and Data Preprocessing performed?

# Insights & Observations

General Information:

- This data set contains in total 33 brands, 11 locations, 5 fuel types, 2 transmission types and 4 owner types, as well as 2041 names. None of our categorical columns contain missing values
- The names column is likely not going to be a useful predictor for this dataset as there are too many unique values. I ended up extracting the Brand info to create a new column (Brand) which will be much easier to work with
- Data cleaning: removed one duplicate value, filled missing values found in Mileage, Engine, Power and Seats columns, dropped New_price column due to a large amount of missing data ( around 86% missing) and dropped records that were missing values of the target variable Price ( around 17% records dropped)
- multiple features in dataset had a high skew, ended up log transforming Engine, Power, New_price and target variable Price
----

Univariate Analysis:

- Cars in this dataset were made in years spanning 1996 to 2019
- The average amount of km driven  is ~57,000
- Both the average and median car Mileage is ~18 kmpl
- The average power displacement is ~1616 bhp
- The average and most frequent amount of seats in a car is 5
- On average, it appears that the Price of a car is sold for is around half of what its original New_price is.
- On average, the sale price of used cars is ~947,000 INR

----

Bivariate Analysis:

- Diesel and Electric cars have higher prices. It could be again that they are higher in demand or perhaps contain more expensive brands
- Automatic cars resale for higher than Manual
- The more owners a car has the lower the resale price it is likely to be sold for. As an increase in owners can be interpreted as greater wear and tear and use on the car, it makes sense that the cars value is implied to decrease every time it changes hands
- Strong positive correlation between Price and Engine, Power and New_price
- Bangalore and Coimbatore locations have cars with slightly higher price values than other locations but there is a lot of variation within each location
- There is no strong pattern in which locations sell which Brands that could explain why certain locations tend to have higher priced cars
- Kolkata and Kochi are the locations that sell the most cars on their first ownership, but all locations have a more than half their cars on their first ownership. In addition, Bangalore ranks fairly high on the pricing yet has the largest amount of second and above owned cars
all locations have a majority of manual transmission cars

## **Building Various Models**


1. What we want to predict is the "Price". We will use the normalized version 'price_log' for modeling.
2. Before we proceed to the model, we'll have to encode categorical features. We will drop categorical features like Name.
3. We'll split the data into train and test, to be able to evaluate the model that we build on the train data.
4. Build Regression models using train data.
5. Evaluate the model performance.
"""

#Create dummy variables for the categorical columns
#drop_first = True used to avoid redundant variables

df_new = pd.get_dummies(
    df_new,
    columns = df_new.select_dtypes(include = 'object').columns,
    drop_first = True,
)

df_new #check work

"""### **Split the Data**

<li>Step1: Seperating the indepdent variables (X) and the dependent variable (y).
<li>Step2: Encode the categorical variables in X using pd.dummies.
<li>Step3: Split the data into train and test using train_test_split.

**Question:**

1. Why we should drop 'Name','Price','price_log','Kilometers_Driven' from X before splitting?
"""

#drop target variable and Kilometers_Driven
x = df_new.drop(['Price', 'Kilometers_Driven'], axis = 1)
y = df_new['Price']

"""# Notes
- We drop Name because it has so many unique values that it is not useful for model prediction and instead extract the Brand Feature frome it
- we drop Price and price_log because those are our target variable
- I am actually not sure why we would drop km_Driven and would like some guidance on that but my guess would be that it had a very low correlation with Price in heatmap and therefore is not expected to have high predictive abilities. That said, I would like some guidance on this as well.
"""

#import modeling functions and split data
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error

#split data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, shuffle = True, random_state = 1)

#check shapes
print("Shape of Training set : ", x_train.shape)
print("Shape of Test set : ", x_test.shape)

"""For Regression Problems, some of the algorithms used are :<br>

**1) Linear Regression** <br>
**2) Ridge / Lasso Regression** <br>
**3) Decision Trees** <br>
**4) Random Forest** <br>
"""

#linear regression
#Function to compute adjusted R-squared

def adj_r2_score(predictors, targets, predictions):
  r2 = r2_score(targets, predictions)
  n = predictors.shape[0]
  k = predictors.shape[1]
  return 1 - ((1 - r2) * (n - 1) / (n - k - 1))

#Function to compute MAPE

def mape_score(targets, predictions):
  return np.mean(np.abs(targets - predictions) / targets) * 100

# Function copute different metrics to check performance of a regression module
def model_performance_regression(model, predictors, target):
  """
  Function to compute different metrics to check regression model performance

  model: regressor
  predictors: independent variables
  target: dependent variable
  """

  pred = model.predict(predictors) #predict using independent variables
  r2 = r2_score(target, pred) #compute R-squared
  adjr2 = adj_r2_score(predictors, target, pred) #compute adjusted R-squared
  rmse = np.sqrt(mean_squared_error(target, pred)) # compute root mean squared error
  mae = mean_absolute_error(target, pred) # compute Mean absolute error
  mape = mape_score(target, pred) # compute mean absolute percentage error

  #Create a df of metrics
  df_perf = pd.DataFrame(
      {
        "RMSE": rmse,
        "MAE": mae,
        "R-squared": r2,
        "Adj. R-squared": adjr2,
        "MAPE": mape,
      },
      index = [0],
  )
  return df_perf

#initialize model

from sklearn.linear_model import LinearRegression

# Initialize the model
model = LinearRegression()

# Fit the model on the training data
model.fit(x_train, y_train)

linear_reg = model_performance_regression(model, x_train, y_train)
linear_reg

# Checking performance on the testing data
linear_reg_test = model_performance_regression(model, x_test, y_test)
linear_reg_test

"""# Notes:

- both our train and test data have similar R-squared and Adj. R-squared values which indicates that the linear regression model is not overfitting on our training data and also that this model fits our data well with an r-squared of ~0.93
- Adj. R-squared of ~0.93 implies that the independent variables are able to explain ~93% variance in the target variable
- MAE indicates that the current model can predict car resale prices within a mean error of ~0.15 units
- RMSE and MAE are fairly close to each other, with RMSE being higher because it penalizes outliers more
- MAPE is ~8% on the test data, indicating that the average difference between the predicted value and the actual value is around ~8%


"""

#import to build prediction models
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor,BaggingRegressor

# To encode categorical variables
from sklearn.preprocessing import LabelEncoder

# For tuning the model
from sklearn.model_selection import GridSearchCV

#ridge regression
ridge_model = Ridge() #creating ridge regression model
ridge_model.fit(x_train, y_train) #fitting the model

#check values using ridge_regression
ridge_reg = model_performance_regression(ridge_model, x_test, y_test) #getting performance metrics on test data
ridge_reg

"""# Notes:
- ridge regression performance metrics show similar results as compared to linear regression
"""

#ridge regression with optimized alpha

folds = KFold(n_splits = 10, shuffle = True, random_state = 1)
params = {'alpha':[0.001, 0.01, 0.1, 0.2, 0.5, 0.9, 1, 5,10,20]}
model = Ridge()
model_cv = GridSearchCV(estimator = model, param_grid = params, scoring = 'r2', cv = folds, return_train_score = True)
model_cv.fit(x_train, y_train)

model_cv.best_params_ #getting optimised params for alpha

ridge_model_tuned = Ridge(alpha=0.1) ##creating Tuned Ridge Regression model using optimised alpha value
ridge_model_tuned.fit(x_train, y_train) # Fitting the data into the tuned model

ridge_reg_tuned = model_performance_regression(ridge_model_tuned, x_test, y_test) #getting performance metrics on test data
ridge_reg_tuned

"""# Notes:
- after using Grid SearchCV, the optimized alpha value is 0.1
- however, after tuning the parameters of ridge regression, the performance parameters do not change which indicates that ridge regression does not help in improving the model
"""

#trying lasso regression

lasso_model = Lasso()
lasso_model.fit(x_train, y_train)

lasso_reg = model_performance_regression(lasso_model, x_test, y_test)
lasso_reg

"""# Notes:

- with lasso regression we see an extreme decrease in our R-squared and Adj. R-squared values as well as an unfavorable increase in our error parameters
- this implies that lasso regression is not the model we want to use for this dataset or that we may need to tune the alpha value
"""

#tuning alpha value

folds = KFold(n_splits=10, shuffle=True, random_state=1)
params = {'alpha':[0.001, 0.01, 0.1, 0.2, 0.5, 0.9, 1, 5,10,20]}
model = Lasso()
model_cv = GridSearchCV(estimator=model, param_grid=params, scoring='r2', cv=folds, return_train_score=True)
model_cv.fit(x_train,y_train)

model_cv.best_params_

lasso_model_tuned = Lasso(alpha=0.001)
lasso_model_tuned.fit(x_train, y_train)

lasso_reg_tuned = model_performance_regression(lasso_model_tuned, x_test, y_test)
lasso_reg_tuned

"""# Notes:

- after using Grid SearchCV, the optimized alpha value is 0.001
- the performance metrics with the tuned alpha value now show similar results to our linear regression and ridge regression models, implying that after adding the lasso regression penalty, the model does not improve
"""

#finally, lets try elastic net regression

elasticnet_model = ElasticNet()
elasticnet_model.fit(x_train, y_train)

#default parameters
elasticnet_reg = model_performance_regression(elasticnet_model, x_test, y_test)
elasticnet_reg

"""# Notes:
- Elastic net regression performs poorly with default values
- lets try tuning them
"""

#lets try optimizing the alpha value l1_ratio

folds = KFold(n_splits=10, shuffle=True, random_state=1)
params = {'alpha':[0.001, 0.01, 0.1, 0.2, 0.5, 0.9],
         'l1_ratio': [0.001, 0.01, 0.02, 0.03, 0.04, 0.05]}
model = ElasticNet()
model_cv = GridSearchCV(estimator=model, param_grid=params, scoring='r2', cv=folds, return_train_score=True)
model_cv.fit(x_train,y_train)

model_cv.best_params_

elasticnet_model_tuned = ElasticNet(alpha=0.001, l1_ratio=0.05)
elasticnet_model_tuned.fit(x_train, y_train)

elasticnet_reg_tuned = model_performance_regression(elasticnet_model_tuned, x_test, y_test)
elasticnet_reg_tuned

"""# Notes:

- after Grid SearchCV, the optimized alpha value is 0.001 and the optimized l1_ratio is 0.05
- the performance metrics show similar but slightly worse results compared to linear regression, ridge regression and lasso regression, implying that using elastic net does not improve the model
"""

#viewing all regression test so far

models= pd.concat([linear_reg_test,ridge_reg,ridge_reg_tuned,lasso_reg,lasso_reg_tuned,elasticnet_reg,
                   elasticnet_reg_tuned], axis=0) #combining all models into a single dataframe
models['Models'] = ['Least Squares', 'Ridge Regression', 'Ridge Regression Tuned', 'Lasso Regression',
                                      'Lasso Regression Tuned', 'Elastic Net Regression',
                    'Elastic Net Regression Tuned'] #adding names of the models as a column to the dataframe
models = models.iloc[:,[5, 0,1,2,3,4]] #ordering names of the models as the first column
models

"""# Observations

- least squares (linear regression) is still giving us the best results compared  to other models
- regularization does not further improve performance metrics

### **Hyperparameter Tuning: Decision Tree**
"""

#build a decision tree model

dt = DecisionTreeRegressor(random_state = 1)
dt.fit(x_train, y_train)

#model perfo on test data
dt_regressor_perf_test = model_performance_regression(dt, x_test, y_test)
dt_regressor_perf_test

"""#  Notes:

- Similar R-squared and Adj. R-squared to linear regression but not as good
"""

#visualize decision tree
from sklearn import tree

features = list(x.columns)

#building the model with maxdepth = 3

dt_regressor_visualize = DecisionTreeRegressor(random_state = 1, max_depth = 3)
dt_regressor_visualize.fit(x_train, y_train)

plt.figure(figsize = (20, 20))
tree.plot_tree(dt_regressor_visualize, feature_names = features, filled = True, fontsize = 10,
               node_ids = True, class_names = True)
plt.show()

"""# Notes:

- the decision tree indicates that year and power have the greatest influence on variance in the target variable

**Feature Importance**
"""

# Plot the feature importance

importances = dt.feature_importances_

columns = x.columns

importance_df = pd.DataFrame(importances, index = columns, columns = ['Importance']).sort_values(by = 'Importance', ascending = False)

plt.figure(figsize = (13, 13))

sns.barplot(x=importance_df.Importance,y=importance_df.index, palette = 'husl');

"""# Observations:

- according to the decision tree, Power is the most important feature, followed by year, engine and then mileage
- The rest of the variables have little or no influence on the length of stay in the hospital in this model

### **Hyperparameter Tuning: Random Forest**
"""

#trying random forest tuning

rf_regressor = RandomForestRegressor(random_state = 1)
rf_regressor.fit(x_train, y_train)

#model performance on test data
rf_regressor_perf_test = model_performance_regression(rf_regressor, x_test, y_test)

rf_regressor_perf_test

"""# Observations

- Random forest model is slightly better than our current linear regression model with an R-squared and Adj. R-Squared of ~0.94 (in comparison to previous ~0.93 value)
- also causes a 1% decrease in MAPE
-RMSE and MAE also decrease slightly implying that this model is slightly better predictor than the least squares method-- very cool!

**Feature Importance**
"""

rf_importances = rf_regressor.feature_importances_

columns = x.columns

rf_importance_df = pd.DataFrame(rf_importances, index = columns, columns = ['Importance']).sort_values(by = 'Importance', ascending = False)

plt.figure(figsize = (13, 13))

sns.barplot(x=rf_importance_df.Importance,y=rf_importance_df.index, palette = 'husl');

"""# Observations:

- much like the prior decision tree feature importance barplot, random forest indicates that Power is the most important feature, followed by year, engine and then mileage

- The rest of the variables have little or no influence on the length of stay in the hospital in this model

# Choosing the final model
"""

#viewing all regression test so far

models= pd.concat([linear_reg_test,ridge_reg,ridge_reg_tuned,lasso_reg,lasso_reg_tuned,elasticnet_reg,
                   elasticnet_reg_tuned, dt_regressor_perf_test, rf_regressor_perf_test], axis=0) #combining all models into a single dataframe
models['Models'] = ['Least Squares', 'Ridge Regression', 'Ridge Regression Tuned', 'Lasso Regression',
                                      'Lasso Regression Tuned', 'Elastic Net Regression',
                    'Elastic Net Regression Tuned', 'Decision Tree Regressor', 'Random Forest Regressor'] #adding names of the models as a column to the dataframe
models = models.iloc[:,[5, 0,1,2,3,4]] #ordering names of the models as the first column
models

"""# Observations:

- the **Random Forest Model** is slightly improved in terms of all parameters, hence we choose the random forest model as our final model.

## **Conclusions and Recommendations**

**1. Comparison of various techniques and their relative performance based on chosen Metric (Measure of success):**
- How do different techniques perform? Which one is performing relatively better? Is there scope to improve the performance further?

# Notes:

- as shown above the Random forest model is the best model by all parameters (RMSE, MAE, R-squared, Adj. R-squared, and MAPE)
- however, linear regression, and the tuned versions of ridge regression and lasso regression also perform fairly strongly as well
- It may be possible to further improve performance by hypertuning the random forest model's hyperparameters as well

**2. Refined insights:**
- What are the most meaningful insights relevant to the problem?

# Business Insights and Recommendations
- Both the Decision Tree Regressor and Random Forest Regressor models indicated that Power followed by Year and then Engine and Mileage hold the strongest influence on Price. When looking for cars to sell the business can prioritize cars with higher bph (Power) and newer cars (Year), and then follow it up with Engine and Mileage values
- In addition, as other features dont carry as much as influence on Price the business does not have to preference for things like fuel_type, transmission, Seat, Brand or Ownership_Type as long as other stronger predictor values are good for the individual car
- That said, it is a shame that the New_price column had to be dropped and it would likely serve the business well to gather more data for that column if they want to increase the predictive power of this model as there was a strong correlation between New_price and Price in the very few non-null values (this column had around 86% of its data missing) in this dataset
-  they business should also consider dedicating more time to recording the value of the Price column for each car sold as in order to do this analysis I dropped around 17% of the records due to not having a value for Price
- it would also be interesting if there was a way to analyze or poll for customer preferences in general to see if there are other features that we could include in this dataset that may boost price values
- Finally, the Random Forest Regressor can predict the resale price of a car with an error of just around 6% variance. This can better aid the business in determining how much their stock will likely sell for and also when buying cars to resell it can help them calculate how much to offer in order to turn a maximum resale profit.

**3. Proposal for the final solution design:**
- What model do you propose to be adopted? Why is this the best solution to adopt?

I propose that the best model is the **Random Forest Regressor model**. The Random Forest Regressor has the **lowest RMSE and MAE**, indicating that the average difference between predicted and actual values is the smallest. It also has a **higher R-squared and Adjusted R-squared**, indicating that the model explains a significant proportion of the variance in the target variable. It also has a low MAPE, indicating that it has a **small average percentage error**.
In addition, it performs well on the test data we can be confident that we are **not overfitting** on the training data.
"""